{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from skforecast.model_selection import backtesting_forecaster\n",
    "from skforecast.model_selection import bayesian_search_forecaster\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../dataset/atm_historical_data_with_features.xlsx')\n",
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "exogenous_variables = ['Holiday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_catboost = df[['Date','Soles_Withdrawn'] + exogenous_variables].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Date', 'Soles_Withdrawn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_catboost_model(params, X_train, y_train, X_test, y_test):\n",
    "    try:\n",
    "        # Desempaquetamos los parámetros\n",
    "        depth, learning_rate, iterations = params\n",
    "        \n",
    "        # Creamos el modelo CatBoost\n",
    "        model = CatBoostRegressor(\n",
    "            depth=int(depth), \n",
    "            learning_rate=float(learning_rate),\n",
    "            iterations=int(iterations),\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Entrenamos el modelo\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Realizamos predicciones\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculamos el error cuadrático medio\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        return mse\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error al ajustar el modelo CatBoost con los parámetros {params}: {e}\")\n",
    "        return float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parámetros de IGAPSOSA\n",
    "# population_size = 20\n",
    "# max_iterations = 100\n",
    "# pso_weight = 0.5  # Peso de PSO\n",
    "# sa_temp = 100  # Temperatura inicial para Simulated Annealing\n",
    "# cooling_rate = 0.9  # Tasa de enfriamiento\n",
    "\n",
    "# # Generar una población inicial aleatoria de soluciones (valores de p, d, q)\n",
    "# def generate_initial_population():\n",
    "#     population = []\n",
    "#     for _ in range(population_size):\n",
    "#         p = random.randint(0, 5)\n",
    "#         d = random.randint(0, 2)\n",
    "#         q = random.randint(0, 5)\n",
    "#         population.append((p, d, q))\n",
    "#     return population\n",
    "\n",
    "# # Función objetivo: minimizar el MSE\n",
    "# def objective_function(solution, data):\n",
    "#     return evaluate_arima_model(data, solution)\n",
    "\n",
    "# # Función de enfriamiento para Simulated Annealing\n",
    "# def cool_temperature(temp, cooling_rate):\n",
    "#     return temp * cooling_rate\n",
    "\n",
    "# # IGAPSOSA: combinación de GA, PSO y SA\n",
    "# def igapsosa(data):\n",
    "#     # Generar población inicial\n",
    "#     population = generate_initial_population()\n",
    "    \n",
    "#     # Evaluar aptitud inicial (fitness)\n",
    "#     fitness = [objective_function(solution, data) for solution in population]\n",
    "#     best_solution = population[np.argmin(fitness)]\n",
    "#     best_fitness = min(fitness)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     temperature = sa_temp  # Inicializar temperatura para Simulated Annealing\n",
    "    \n",
    "#     for iteration in range(max_iterations):\n",
    "#         # Paso 1: Algoritmo Genético (Selección y Cruce)\n",
    "#         selected_population = random.sample(population, population_size // 2)\n",
    "#         new_population = selected_population.copy()\n",
    "\n",
    "#         for i in range(0, len(selected_population), 2):\n",
    "#             parent1 = selected_population[i]\n",
    "#             parent2 = selected_population[(i + 1) % len(selected_population)]\n",
    "            \n",
    "#             # Aplicamos cruce (crossover)\n",
    "#             child1 = (parent1[0], parent2[1], parent1[2])\n",
    "#             child2 = (parent2[0], parent1[1], parent2[2])\n",
    "#             new_population.append(child1)\n",
    "#             new_population.append(child2)\n",
    "        \n",
    "#         # Paso 2: PSO (actualización de velocidad y posición)\n",
    "#         for i in range(len(new_population)):\n",
    "#             if random.random() < pso_weight:\n",
    "#                 new_population[i] = tuple(random.choice(new_population))\n",
    "        \n",
    "#         # Paso 3: Recocido Simulado (SA)\n",
    "#         for i in range(len(new_population)):\n",
    "#             candidate_fitness = objective_function(new_population[i], data)\n",
    "#             if candidate_fitness < fitness[i] or random.random() < np.exp((fitness[i] - candidate_fitness) / temperature):\n",
    "#                 population[i] = new_population[i]\n",
    "#                 fitness[i] = candidate_fitness\n",
    "        \n",
    "#         # Actualizar la mejor solución global\n",
    "#         current_best = min(fitness)\n",
    "#         if current_best < best_fitness:\n",
    "#             best_fitness = current_best\n",
    "#             best_solution = population[np.argmin(fitness)]\n",
    "        \n",
    "#         # Reducir la temperatura\n",
    "#         temperature = cool_temperature(temperature, cooling_rate)\n",
    "    \n",
    "#     return best_solution, best_fitness\n",
    "\n",
    "# # Aplicamos IGAPSOSA al DataFrame 'df' para optimizar los parámetros ARIMA\n",
    "# best_params, best_score = igapsosa(df['Soles_Withdrawn'].values)\n",
    "# print(f\"Mejores parámetros: {best_params}\")\n",
    "# print(f\"Mejor MSE: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros: [9.002376021156365, 0.0688220742357123, 201.51952771548463]\n",
      "Mejor MSE: 111684.08197728037\n"
     ]
    }
   ],
   "source": [
    "# Función de optimización (IGAPSOSA)\n",
    "def igapsosa(data, target):\n",
    "    # Parámetros iniciales del algoritmo IGAPSOSA\n",
    "    # Ejemplo: Hiperparámetros que queremos optimizar\n",
    "    param_bounds = {\n",
    "        'depth': (4, 10),  # Profundidad del árbol\n",
    "        'learning_rate': (0.01, 0.3),  # Tasa de aprendizaje\n",
    "        'iterations': (100, 1000)  # Número de iteraciones\n",
    "    }\n",
    "    \n",
    "    # Función para generar la población inicial\n",
    "    def generate_initial_population(pop_size=10):\n",
    "        population = []\n",
    "        for _ in range(pop_size):\n",
    "            depth = np.random.uniform(*param_bounds['depth'])\n",
    "            learning_rate = np.random.uniform(*param_bounds['learning_rate'])\n",
    "            iterations = np.random.uniform(*param_bounds['iterations'])\n",
    "            population.append([depth, learning_rate, iterations])\n",
    "        return population\n",
    "\n",
    "    # Dividimos el dataset en entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Generamos la población inicial\n",
    "    population = generate_initial_population()\n",
    "\n",
    "    # Evaluamos el fitness inicial (MSE)\n",
    "    fitness = [evaluate_catboost_model(solution, X_train, y_train, X_test, y_test) for solution in population]\n",
    "    best_solution = population[np.argmin(fitness)]\n",
    "    best_fitness = min(fitness)\n",
    "\n",
    "    # Aquí puedes seguir aplicando los operadores de IGAPSOSA (crossover, mutation, etc.)\n",
    "    # Para simplificar, solo estamos generando una población inicial y evaluando su aptitud\n",
    "\n",
    "    return best_solution, best_fitness\n",
    "\n",
    "# Creamos un DataFrame de ejemplo\n",
    "df = pd.DataFrame({\n",
    "    'Date': pd.date_range(start='2020-01-01', periods=100, freq='D'),\n",
    "    'Soles_Withdrawn': np.random.rand(100) * 1000\n",
    "})\n",
    "\n",
    "# Llamamos a IGAPSOSA con los datos\n",
    "X = df.drop(columns=['Soles_Withdrawn'])\n",
    "y = df['Soles_Withdrawn'].values\n",
    "\n",
    "best_params, best_score = igapsosa(X, y)\n",
    "print(f\"Mejores parámetros: {best_params}\")\n",
    "print(f\"Mejor MSE: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13.4\n"
     ]
    }
   ],
   "source": [
    "import mlforecast\n",
    "print(mlforecast.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las variables exógenas\n",
    "exog_cols = ['tu_variable_exogena_1', 'tu_variable_exogena_2']  # Ajusta según tu dataset\n",
    "fin_validacion = int(len(df) * 0.8)  # División 80-20\n",
    "datos_train = df.iloc[:fin_validacion]\n",
    "datos_validacion = df.iloc[fin_validacion:]\n",
    "\n",
    "# One hot encoding\n",
    "one_hot_encoder = make_column_transformer(\n",
    "    (OneHotEncoder(sparse_output=False, drop='if_binary'), \n",
    "     make_column_selector(dtype_exclude=np.number)),\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False\n",
    ").set_output(transform='pandas')\n",
    "\n",
    "# Crear forecaster\n",
    "forecaster = ForecasterAutoreg(\n",
    "    regressor=CatBoostRegressor(\n",
    "        random_state=123,\n",
    "        silent=True,\n",
    "        allow_writing_files=False,\n",
    "        boosting_type='Plain',\n",
    "        leaf_estimation_iterations=3,\n",
    "    ),\n",
    "    lags=24,  # Ajusta según sea necesario\n",
    "    transformer_exog=one_hot_encoder\n",
    ")\n",
    "\n",
    "# Búsqueda de hiperparámetros\n",
    "lags_grid = [48, 72, [1, 2, 3, 23, 24, 25, 167, 168, 169]]\n",
    "\n",
    "def search_space(trial):\n",
    "    search_space = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10, step=1),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1),\n",
    "        'lags': trial.suggest_categorical('lags', lags_grid)\n",
    "    }\n",
    "    return search_space\n",
    "\n",
    "# Ejecutar búsqueda bayesiana de hiperparámetros\n",
    "results_search, frozen_trial = bayesian_search_forecaster(\n",
    "    forecaster=forecaster,\n",
    "    y=datos_train['Soles_withdrawn'],\n",
    "    exog=datos_train[exog_cols],\n",
    "    search_space=search_space,\n",
    "    steps=36,\n",
    "    refit=False,\n",
    "    metric='mean_absolute_error',\n",
    "    initial_train_size=len(datos_train),\n",
    "    fixed_train_size=False,\n",
    "    n_trials=20,\n",
    "    random_state=123,\n",
    "    return_best=True,\n",
    "    n_jobs='auto',\n",
    "    verbose=False,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Backtesting con datos de test\n",
    "metrica_catboost, predicciones = backtesting_forecaster(\n",
    "    forecaster=forecaster,\n",
    "    y=df['Soles_withdrawn'],\n",
    "    exog=df[exog_cols],\n",
    "    initial_train_size=len(datos_train),\n",
    "    fixed_train_size=False,\n",
    "    steps=36,\n",
    "    refit=False,\n",
    "    metric='mean_absolute_error',\n",
    "    n_jobs='auto',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Mejores parámetros:\", frozen_trial.params)\n",
    "print(\"Mejor MAE:\", metrica_catboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exog_cols = ['Holiday']  # Ajusta según tu dataset\n",
    "fin_validacion = int(len(df_catboost) * 0.8)  # División 80-20\n",
    "datos_train = df_catboost.iloc[:fin_validacion]\n",
    "datos_validacion = df_catboost.iloc[fin_validacion:]\n",
    "\n",
    "# One hot encoding\n",
    "one_hot_encoder = make_column_transformer(\n",
    "    (OneHotEncoder(sparse_output=False, drop='if_binary'), \n",
    "     make_column_selector(dtype_exclude=np.number)),\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False\n",
    ").set_output(transform='pandas')\n",
    "\n",
    "# Crear forecaster\n",
    "forecaster = ForecasterAutoreg(\n",
    "    regressor=CatBoostRegressor(\n",
    "        random_state=123,\n",
    "        silent=True,\n",
    "        allow_writing_files=False,\n",
    "        boosting_type='Plain',\n",
    "        leaf_estimation_iterations=3,\n",
    "    ),\n",
    "    lags=24,  # Ajusta según sea necesario\n",
    ")\n",
    "\n",
    "# Búsqueda de hiperparámetros\n",
    "search_space = {\n",
    "    'n_estimators': (100, 1000),\n",
    "    'max_depth': (3, 10),\n",
    "    'learning_rate': (0.01, 1),\n",
    "}\n",
    "\n",
    "initial_train_size = len(datos_train)\n",
    "\n",
    "def search_space(trial):\n",
    "    return {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10, step=1),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1),\n",
    "    }\n",
    "\n",
    "# Ejecutar búsqueda bayesiana de hiperparámetros\n",
    "results_search = bayesian_search_forecaster(\n",
    "    forecaster=forecaster,\n",
    "    y=datos_train['Soles_Withdrawn'],\n",
    "    exog=datos_train[exog_cols],\n",
    "    search_space=search_space,  # Ahora es una función\n",
    "    steps=36,\n",
    "    refit=False,\n",
    "    metric='mean_absolute_error',\n",
    "    n_trials=20,\n",
    "    random_state=123,\n",
    "    verbose=False,\n",
    "    initial_train_size=initial_train_size\n",
    ")\n",
    "\n",
    "# Backtesting con datos de test\n",
    "metrica_catboost, predicciones = backtesting_forecaster(\n",
    "    forecaster=forecaster,\n",
    "    y=df_catboost['Soles_Withdrawn'],\n",
    "    exog=df_catboost[exog_cols],\n",
    "    initial_train_size=initial_train_size,\n",
    "    fixed_train_size=False,\n",
    "    steps=36,\n",
    "    refit=False,\n",
    "    metric='mean_absolute_error',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Mejores parámetros:\", results_search['best_params'])\n",
    "print(\"Mejor MAE:\", metrica_catboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 8247.27: 100%|██████████| 20/20 [03:04<00:00,  9.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n",
      "  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] \n",
      "  Parameters: {'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.016388840477631006}\n",
      "  Backtesting metric: 8247.274671481195\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:03<00:00, 11.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# Supongamos que df_catboost es tu DataFrame con los datos\n",
    "exog_cols = ['Holiday']  # Usar directamente si ya está codificada en 0 y 1\n",
    "fin_validacion = int(len(df_catboost) * 0.8)  # División 80-20\n",
    "datos_train = df_catboost.iloc[:fin_validacion]\n",
    "datos_validacion = df_catboost.iloc[fin_validacion:]\n",
    "\n",
    "# Crear forecaster\n",
    "forecaster = ForecasterAutoreg(\n",
    "    regressor=CatBoostRegressor(\n",
    "        random_state=123,\n",
    "        silent=True,\n",
    "        allow_writing_files=False,\n",
    "        boosting_type='Plain',\n",
    "        leaf_estimation_iterations=3,\n",
    "    ),\n",
    "    lags=24,  # Ajusta según sea necesario\n",
    ")\n",
    "\n",
    "# Búsqueda de hiperparámetros\n",
    "search_space = {\n",
    "    'n_estimators': (100, 1000),\n",
    "    'max_depth': (3, 10),\n",
    "    'learning_rate': (0.01, 1),\n",
    "}\n",
    "\n",
    "# Asegúrate de que initial_train_size sea menor que la longitud de datos_train\n",
    "initial_train_size = int(len(datos_train) * 0.8)  # Por ejemplo, 80% de datos_train\n",
    "\n",
    "def search_space_function(trial):\n",
    "    return {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10, step=1),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1),\n",
    "    }\n",
    "\n",
    "# Ejecutar búsqueda bayesiana de hiperparámetros\n",
    "results_search = bayesian_search_forecaster(\n",
    "    forecaster=forecaster,\n",
    "    y=datos_train['Soles_Withdrawn'],\n",
    "    exog=datos_train[exog_cols],\n",
    "    search_space=search_space_function,  # Usar la función de búsqueda\n",
    "    steps=36,\n",
    "    refit=False,\n",
    "    metric='mean_absolute_error',\n",
    "    n_trials=20,\n",
    "    random_state=123,\n",
    "    verbose=False,\n",
    "    initial_train_size=initial_train_size\n",
    ")\n",
    "\n",
    "# Backtesting con datos de test\n",
    "metrica_catboost, predicciones = backtesting_forecaster(\n",
    "    forecaster=forecaster,\n",
    "    y=df_catboost['Soles_Withdrawn'],\n",
    "    exog=df_catboost[exog_cols],\n",
    "    initial_train_size=initial_train_size,\n",
    "    fixed_train_size=False,\n",
    "    steps=36,\n",
    "    refit=False,\n",
    "    metric='mean_absolute_error',\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de búsqueda: (                                                 lags  \\\n",
      "11  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "10  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "14  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "12  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "17  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "15  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "16  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "18  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "5   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "2   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "8   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "4   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "1   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "0   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "6   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "7   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "9   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "3   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "13  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "19  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "\n",
      "                                               params  mean_absolute_error  \\\n",
      "11  {'n_estimators': 100, 'max_depth': 10, 'learni...          8247.274671   \n",
      "10  {'n_estimators': 100, 'max_depth': 10, 'learni...          8253.809925   \n",
      "14  {'n_estimators': 200, 'max_depth': 10, 'learni...          8278.400565   \n",
      "12  {'n_estimators': 100, 'max_depth': 10, 'learni...          8280.594006   \n",
      "17  {'n_estimators': 100, 'max_depth': 9, 'learnin...          8404.861452   \n",
      "15  {'n_estimators': 300, 'max_depth': 9, 'learnin...          8475.967215   \n",
      "16  {'n_estimators': 200, 'max_depth': 9, 'learnin...          8551.432404   \n",
      "18  {'n_estimators': 400, 'max_depth': 7, 'learnin...          8765.192422   \n",
      "5   {'n_estimators': 800, 'max_depth': 4, 'learnin...          8901.789023   \n",
      "2   {'n_estimators': 1000, 'max_depth': 8, 'learni...          8924.993483   \n",
      "8   {'n_estimators': 800, 'max_depth': 5, 'learnin...          9003.585257   \n",
      "4   {'n_estimators': 500, 'max_depth': 3, 'learnin...          9087.572315   \n",
      "1   {'n_estimators': 600, 'max_depth': 8, 'learnin...          9139.345845   \n",
      "0   {'n_estimators': 700, 'max_depth': 5, 'learnin...          9168.031052   \n",
      "6   {'n_estimators': 600, 'max_depth': 7, 'learnin...          9193.960588   \n",
      "7   {'n_estimators': 900, 'max_depth': 8, 'learnin...          9270.033818   \n",
      "9   {'n_estimators': 300, 'max_depth': 5, 'learnin...          9566.938546   \n",
      "3   {'n_estimators': 400, 'max_depth': 5, 'learnin...          9635.525927   \n",
      "13  {'n_estimators': 100, 'max_depth': 10, 'learni...         10241.252963   \n",
      "19  {'n_estimators': 200, 'max_depth': 10, 'learni...         10464.260487   \n",
      "\n",
      "    n_estimators  max_depth  learning_rate  \n",
      "11         100.0       10.0       0.016389  \n",
      "10         100.0       10.0       0.060453  \n",
      "14         200.0       10.0       0.014754  \n",
      "12         100.0       10.0       0.022880  \n",
      "17         100.0        9.0       0.104140  \n",
      "15         300.0        9.0       0.143614  \n",
      "16         200.0        9.0       0.310357  \n",
      "18         400.0        7.0       0.283469  \n",
      "5          800.0        4.0       0.183697  \n",
      "2         1000.0        8.0       0.486123  \n",
      "8          800.0        5.0       0.368171  \n",
      "4          500.0        3.0       0.404064  \n",
      "1          600.0        8.0       0.428875  \n",
      "0          700.0        5.0       0.234583  \n",
      "6          600.0        7.0       0.638057  \n",
      "7          900.0        8.0       0.614913  \n",
      "9          300.0        5.0       0.634666  \n",
      "3          400.0        5.0       0.731759  \n",
      "13         100.0       10.0       0.963596  \n",
      "19         200.0       10.0       0.883333  , FrozenTrial(number=11, state=TrialState.COMPLETE, values=[8247.274671481195], datetime_start=datetime.datetime(2024, 9, 27, 10, 27, 24, 267447), datetime_complete=datetime.datetime(2024, 9, 27, 10, 27, 33, 101288), params={'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.016388840477631006}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'n_estimators': IntDistribution(high=1000, log=False, low=100, step=100), 'max_depth': IntDistribution(high=10, log=False, low=3, step=1), 'learning_rate': FloatDistribution(high=1.0, log=False, low=0.01, step=None)}, trial_id=11, value=None))\n"
     ]
    }
   ],
   "source": [
    "print(\"Resultados de búsqueda:\", results_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros:                                                  lags  \\\n",
      "11  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "10  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "14  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "12  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "17  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "15  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "16  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "18  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "5   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "2   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "8   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "4   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "1   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "0   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "6   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "7   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "9   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "3   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "13  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "19  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "\n",
      "                                               params  mean_absolute_error  \\\n",
      "11  {'n_estimators': 100, 'max_depth': 10, 'learni...          8247.274671   \n",
      "10  {'n_estimators': 100, 'max_depth': 10, 'learni...          8253.809925   \n",
      "14  {'n_estimators': 200, 'max_depth': 10, 'learni...          8278.400565   \n",
      "12  {'n_estimators': 100, 'max_depth': 10, 'learni...          8280.594006   \n",
      "17  {'n_estimators': 100, 'max_depth': 9, 'learnin...          8404.861452   \n",
      "15  {'n_estimators': 300, 'max_depth': 9, 'learnin...          8475.967215   \n",
      "16  {'n_estimators': 200, 'max_depth': 9, 'learnin...          8551.432404   \n",
      "18  {'n_estimators': 400, 'max_depth': 7, 'learnin...          8765.192422   \n",
      "5   {'n_estimators': 800, 'max_depth': 4, 'learnin...          8901.789023   \n",
      "2   {'n_estimators': 1000, 'max_depth': 8, 'learni...          8924.993483   \n",
      "8   {'n_estimators': 800, 'max_depth': 5, 'learnin...          9003.585257   \n",
      "4   {'n_estimators': 500, 'max_depth': 3, 'learnin...          9087.572315   \n",
      "1   {'n_estimators': 600, 'max_depth': 8, 'learnin...          9139.345845   \n",
      "0   {'n_estimators': 700, 'max_depth': 5, 'learnin...          9168.031052   \n",
      "6   {'n_estimators': 600, 'max_depth': 7, 'learnin...          9193.960588   \n",
      "7   {'n_estimators': 900, 'max_depth': 8, 'learnin...          9270.033818   \n",
      "9   {'n_estimators': 300, 'max_depth': 5, 'learnin...          9566.938546   \n",
      "3   {'n_estimators': 400, 'max_depth': 5, 'learnin...          9635.525927   \n",
      "13  {'n_estimators': 100, 'max_depth': 10, 'learni...         10241.252963   \n",
      "19  {'n_estimators': 200, 'max_depth': 10, 'learni...         10464.260487   \n",
      "\n",
      "    n_estimators  max_depth  learning_rate  \n",
      "11         100.0       10.0       0.016389  \n",
      "10         100.0       10.0       0.060453  \n",
      "14         200.0       10.0       0.014754  \n",
      "12         100.0       10.0       0.022880  \n",
      "17         100.0        9.0       0.104140  \n",
      "15         300.0        9.0       0.143614  \n",
      "16         200.0        9.0       0.310357  \n",
      "18         400.0        7.0       0.283469  \n",
      "5          800.0        4.0       0.183697  \n",
      "2         1000.0        8.0       0.486123  \n",
      "8          800.0        5.0       0.368171  \n",
      "4          500.0        3.0       0.404064  \n",
      "1          600.0        8.0       0.428875  \n",
      "0          700.0        5.0       0.234583  \n",
      "6          600.0        7.0       0.638057  \n",
      "7          900.0        8.0       0.614913  \n",
      "9          300.0        5.0       0.634666  \n",
      "3          400.0        5.0       0.731759  \n",
      "13         100.0       10.0       0.963596  \n",
      "19         200.0       10.0       0.883333  \n",
      "Mejor MAE: FrozenTrial(number=11, state=TrialState.COMPLETE, values=[8247.274671481195], datetime_start=datetime.datetime(2024, 9, 27, 10, 27, 24, 267447), datetime_complete=datetime.datetime(2024, 9, 27, 10, 27, 33, 101288), params={'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.016388840477631006}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'n_estimators': IntDistribution(high=1000, log=False, low=100, step=100), 'max_depth': IntDistribution(high=10, log=False, low=3, step=1), 'learning_rate': FloatDistribution(high=1.0, log=False, low=0.01, step=None)}, trial_id=11, value=None)\n"
     ]
    }
   ],
   "source": [
    "# Suponiendo que el primer elemento del tuple es un dict con los mejores parámetros\n",
    "best_params = results_search[0]  # Acceder al primer elemento (ajusta según sea necesario)\n",
    "best_mae = results_search[1]  # Acceder al segundo elemento (ajusta según sea necesario)\n",
    "\n",
    "print(\"Mejores parámetros:\", best_params)\n",
    "print(\"Mejor MAE:\", best_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGAPSOSA - Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particle:\n",
    "    def __init__(self, bounds):\n",
    "        self.position = np.random.uniform(bounds[:, 0], bounds[:, 1])\n",
    "        self.velocity = np.random.uniform(-1, 1, size=len(bounds))\n",
    "        self.best_position = self.position.copy()\n",
    "        self.best_score = float('inf')\n",
    "\n",
    "class IGAPSOSA:\n",
    "    def __init__(self, objective_function, bounds, num_particles=30, max_iterations=100, w=0.5, c1=1.5, c2=1.5):\n",
    "        self.objective_function = objective_function\n",
    "        self.bounds = bounds\n",
    "        self.num_particles = num_particles\n",
    "        self.max_iterations = max_iterations\n",
    "        self.w = w\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "\n",
    "    def optimize(self):\n",
    "        particles = [Particle(self.bounds) for _ in range(self.num_particles)]\n",
    "        global_best_position = None\n",
    "        global_best_score = float('inf')\n",
    "\n",
    "        for iteration in range(self.max_iterations):\n",
    "            for particle in particles:\n",
    "                score = self.objective_function(particle.position)\n",
    "\n",
    "                if score < particle.best_score:\n",
    "                    particle.best_score = score\n",
    "                    particle.best_position = particle.position.copy()\n",
    "\n",
    "                if score < global_best_score:\n",
    "                    global_best_score = score\n",
    "                    global_best_position = particle.position.copy()\n",
    "\n",
    "            for particle in particles:\n",
    "                r1, r2 = np.random.rand(2)\n",
    "                particle.velocity = (self.w * particle.velocity +\n",
    "                                     self.c1 * r1 * (particle.best_position - particle.position) +\n",
    "                                     self.c2 * r2 * (global_best_position - particle.position))\n",
    "                \n",
    "                particle.position += particle.velocity\n",
    "                particle.position = np.clip(particle.position, self.bounds[:, 0], self.bounds[:, 1])\n",
    "\n",
    "        return global_best_position, global_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(params):\n",
    "    n_estimators = int(params[0])\n",
    "    max_depth = int(params[1])\n",
    "    learning_rate = params[2]  # Se mantiene como float\n",
    "    \n",
    "    model = CatBoostRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        random_state=123,\n",
    "        silent=True,\n",
    "        allow_writing_files=False\n",
    "    )\n",
    "    \n",
    "    model.fit(datos_train[exog_cols], datos_train['Soles_Withdrawn'])\n",
    "    predictions = model.predict(datos_validacion[exog_cols])\n",
    "    mae = mean_absolute_error(datos_validacion['Soles_Withdrawn'], predictions)\n",
    "    \n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "exog_cols = ['Holiday']  # Ajusta según tu dataset\n",
    "fin_validacion = int(len(df_catboost) * 0.8)  # División 80-20\n",
    "datos_train = df_catboost.iloc[:fin_validacion]\n",
    "datos_validacion = df_catboost.iloc[fin_validacion:]\n",
    "\n",
    "# Definir los límites de los hiperparámetros\n",
    "bounds = np.array([[100, 1000],  # n_estimators\n",
    "                   [3, 10],     # max_depth\n",
    "                   [0.01, 1]])  # learning_rate\n",
    "\n",
    "# Ejecutar el optimizador IGAPSOSA\n",
    "optimizer = IGAPSOSA(objective_function, bounds, num_particles=30, max_iterations=100)\n",
    "best_position, best_score = optimizer.optimize()\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(\"n_estimators:\", int(best_position[0]))\n",
    "print(\"max_depth:\", int(best_position[1]))\n",
    "print(\"learning_rate:\", best_position[2])\n",
    "print(\"Mejor MAE:\", best_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
